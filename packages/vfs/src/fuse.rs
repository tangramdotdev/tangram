use {
	self::sys::{
		fuse_attr, fuse_attr_out, fuse_batch_forget_in, fuse_entry_out, fuse_flush_in,
		fuse_forget_in, fuse_forget_one, fuse_getattr_in, fuse_getxattr_in, fuse_getxattr_out,
		fuse_in_header, fuse_init_in, fuse_init_out, fuse_open_in, fuse_open_out, fuse_out_header,
		fuse_read_in, fuse_release_in,
	},
	crate::{FileType, Provider, Result},
	bytes::Bytes,
	num::ToPrimitive as _,
	std::{
		ffi::CString,
		io::{Error, IoSlice},
		ops::Deref,
		os::{
			fd::{AsFd as _, AsRawFd as _, OwnedFd},
			unix::process::CommandExt as _,
		},
		path::Path,
		sync::{
			Arc, Mutex,
			atomic::{AtomicBool, Ordering},
		},
	},
	sys::{FUSE_KERNEL_MINOR_VERSION, FUSE_KERNEL_VERSION, fuse_interrupt_in},
};

#[allow(warnings)]
pub mod sys;

// FUSE device ioctl opcode for passthrough backing registration. This is defined as a C macro in
// fuse.h and is not generated by bindgen.
const FUSE_DEV_IOC_BACKING_OPEN: rustix::ioctl::Opcode =
	rustix::ioctl::opcode::write::<fuse_backing_map>(229, 1); // _IOW(229, 1, struct fuse_backing_map)

// Errno constants for FUSE error handling.
const ENODATA: i32 = rustix::io::Errno::NODATA.raw_os_error();
const ENOENT: i32 = rustix::io::Errno::NOENT.raw_os_error();
const ENOSYS: i32 = rustix::io::Errno::NOSYS.raw_os_error();
const ERANGE: i32 = rustix::io::Errno::RANGE.raw_os_error();

// The size of the payload buffer for each uring cmd entry. This must be at least
// `max(FUSE_MIN_READ_BUFFER, max_write, max_pages * PAGE_SIZE)` as computed by the kernel in
// `fuse_uring_create`. With the default `max_pages` of 32 and a page size of 4096, the kernel
// expects at least 128KB.
const URING_CMD_PAYLOAD_SIZE: usize = 128 * 1024;

// Token for eventfd completions. Entry tokens use 0..total_entries-1.
const EVENTFD_TOKEN: u64 = u64::MAX;

// Linux poll event constant.
const POLLIN: u32 = 0x0001;

// Mode type constants for FUSE attribute encoding.
const S_IFDIR: u32 = 0o040_000;
const S_IFREG: u32 = 0o100_000;
const S_IFLNK: u32 = 0o120_000;

// Local fixed-size versions of fuse_dirent and fuse_direntplus. The bindgen versions have a
// flexible array member for the name, which prevents them from implementing Copy and zerocopy
// traits. These are used as headers during serialization; the name bytes are appended separately.
#[repr(C)]
#[derive(Clone, Copy, Debug)]
struct fuse_dirent {
	ino: u64,
	off: u64,
	namelen: u32,
	type_: u32,
}

#[repr(C)]
#[derive(Clone, Copy, Debug)]
struct fuse_direntplus {
	entry_out: fuse_entry_out,
	dirent: fuse_dirent,
}

// Local fuse_backing_map for the passthrough ioctl.
#[repr(C)]
struct fuse_backing_map {
	fd: i32,
	flags: u32,
	padding: u64,
}

/// Custom ioctl for `FUSE_DEV_IOC_BACKING_OPEN` that captures the backing id from the return value.
struct BackingOpenIoctl(fuse_backing_map);

unsafe impl rustix::ioctl::Ioctl for BackingOpenIoctl {
	type Output = i32;

	const IS_MUTATING: bool = false;

	fn opcode(&self) -> rustix::ioctl::Opcode {
		FUSE_DEV_IOC_BACKING_OPEN
	}

	fn as_ptr(&mut self) -> *mut std::ffi::c_void {
		std::ptr::addr_of_mut!(self.0).cast()
	}

	unsafe fn output_from_ptr(
		out: rustix::ioctl::IoctlOutput,
		_ptr: *mut std::ffi::c_void,
	) -> rustix::io::Result<i32> {
		Ok(out)
	}
}

/// An iovec for the uring cmd buffer entry. The kernel reads `iov_base`/`iov_len` to locate the
/// header and payload buffers.
#[repr(C)]
struct Iovec {
	iov_base: *mut std::ffi::c_void,
	iov_len: usize,
}

/// A buffer entry for FUSE `io_uring` command mode. Each entry holds a request header and a
/// payload buffer. The iov array points to these two regions and its address is passed to the
/// kernel via the SQE's addr field. All three heap allocations (header, payload, iov) have stable
/// addresses regardless of whether the `UringCmdEntry` itself is moved.
struct UringCmdEntry {
	header: Box<sys::fuse_uring_req_header>,
	payload: Vec<u8>,
	iov: Box<[Iovec; 2]>,
	qid: u16,
}

impl UringCmdEntry {
	fn new(qid: u16) -> Self {
		// Safety: fuse_uring_req_header is repr(C) and all-zeros is a valid state.
		let mut header = Box::new(unsafe { std::mem::zeroed::<sys::fuse_uring_req_header>() });
		let mut payload = vec![0u8; URING_CMD_PAYLOAD_SIZE];
		let iov = Box::new([
			Iovec {
				iov_base: std::ptr::from_mut::<sys::fuse_uring_req_header>(header.as_mut()).cast(),
				iov_len: std::mem::size_of::<sys::fuse_uring_req_header>(),
			},
			Iovec {
				iov_base: payload.as_mut_ptr().cast(),
				iov_len: URING_CMD_PAYLOAD_SIZE,
			},
		]);
		Self {
			header,
			payload,
			iov,
			qid,
		}
	}
}

pub struct Server<P>(Arc<Inner<P>>);

pub struct Inner<P> {
	provider: P,
	task: Mutex<Option<tangram_futures::task::Shared<()>>>,
}

/// A request.
#[derive(Clone, Debug)]
struct Request {
	header: sys::fuse_in_header,
	data: RequestData,
}

/// A request's data.
#[derive(Clone, Debug)]
enum RequestData {
	BatchForget(Vec<fuse_forget_one>),
	Destroy,
	Flush(sys::fuse_flush_in),
	Forget(sys::fuse_forget_in),
	GetAttr(sys::fuse_getattr_in),
	GetXattr(sys::fuse_getxattr_in, CString),
	Init(sys::fuse_init_in),
	ListXattr(sys::fuse_getxattr_in),
	Lookup(CString),
	Open(sys::fuse_open_in),
	OpenDir(sys::fuse_open_in),
	Read(sys::fuse_read_in),
	ReadDir(sys::fuse_read_in),
	ReadDirPlus(sys::fuse_read_in),
	ReadLink,
	Release(sys::fuse_release_in),
	ReleaseDir(sys::fuse_release_in),
	Statfs,
	Statx(sys::fuse_statx_in),
	Interrupt(sys::fuse_interrupt_in),
	Unsupported(u32),
}

/// A response.
#[derive(Clone, Debug)]
enum Response {
	Flush,
	GetAttr(sys::fuse_attr_out),
	GetXattr(Vec<u8>),
	Init(sys::fuse_init_out),
	ListXattr(Vec<u8>),
	Lookup(sys::fuse_entry_out),
	Open(sys::fuse_open_out),
	OpenDir(sys::fuse_open_out),
	Read(Bytes),
	ReadDir(Vec<u8>),
	ReadDirPlus(Vec<u8>),
	ReadLink(CString),
	Release,
	ReleaseDir,
	Statfs(sys::fuse_statfs_out),
	Statx(sys::fuse_statx_out),
}

/// The result of trying to handle a request synchronously.
enum SyncResult {
	/// The request was handled and a response should be sent.
	Response(Result<Option<Response>>),
	/// The request does not produce a response (e.g., FORGET).
	NoResponse,
	/// The request needs async handling.
	NeedAsync(Request),
}

impl<P> Server<P>
where
	P: Provider + Send + Sync + 'static,
{
	pub async fn start(provider: P, path: &Path, workers: usize) -> Result<Self> {
		// Unmount.
		unmount(path).await.ok();

		// Mount.
		let fd = Self::mount(path)
			.await
			.inspect_err(|error| tracing::error!(%error, "failed to mount"))?;

		// Perform the INIT handshake synchronously on the original fd.
		Self::init_handshake(fd.as_ref())?;

		// Create the server.
		let server = Self(Arc::new(Inner {
			provider,
			task: Mutex::new(None),
		}));

		// Create the stop flag.
		let stop_flag = Arc::new(AtomicBool::new(false));

		// Capture the tokio runtime handle so the worker thread can spawn async tasks.
		let runtime_handle = tokio::runtime::Handle::current();

		// Spawn a single worker thread that runs the uring cmd loop.
		let server_clone = server.clone();
		let stop_flag_clone = stop_flag.clone();
		let fd_clone = fd.clone();
		let runtime_handle = runtime_handle.clone();
		let handle = std::thread::spawn(move || {
			let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
				worker_thread_loop_uring_cmd(
					&server_clone,
					&fd_clone,
					&stop_flag_clone,
					workers,
					&runtime_handle,
				);
			}));
			if let Err(error) = result {
				let message = error
					.downcast_ref::<String>()
					.map(String::as_str)
					.or(error.downcast_ref::<&str>().copied());
				tracing::error!(?message, "worker thread panicked");
			}
		});

		// Spawn the lifecycle task.
		let path = path.to_owned();
		let task = tangram_futures::task::Shared::spawn(move |stop| {
			let stop_flag = stop_flag.clone();
			async move {
				stop.wait().await;
				stop_flag.store(true, Ordering::SeqCst);
				unmount(&path)
					.await
					.inspect_err(|error| tracing::error!(%error, "failed to unmount"))
					.ok();
				// Join the worker thread.
				tokio::task::spawn_blocking(move || {
					handle.join().ok();
				})
				.await
				.ok();
			}
		});
		server.task.lock().unwrap().replace(task);

		Ok(server)
	}

	pub fn stop(&self) {
		self.task.lock().unwrap().as_ref().unwrap().stop();
	}

	pub async fn wait(&self) {
		let task = self.task.lock().unwrap().clone().unwrap();
		task.wait().await.unwrap();
	}

	/// Perform the FUSE INIT handshake on the given fd.
	fn init_handshake(fd: &OwnedFd) -> Result<()> {
		// Read the INIT request.
		let mut buffer = vec![0u8; 1024 * 1024 + 4096];
		let n = rustix::io::read(fd, &mut buffer[..])?;
		let request = Self::deserialize_request(&buffer[..n])?;
		let RequestData::Init(init_in) = request.data else {
			return Err(Error::other("expected INIT request"));
		};

		// Check the kernel's flags.
		let kernel_flags =
			init_in.flags.to_u64().unwrap() | (init_in.flags2.to_u64().unwrap() << 32);
		let passthrough = kernel_flags & sys::FUSE_PASSTHROUGH != 0;

		// Build our desired flags.
		let mut flags: u64 = u64::from(
			sys::FUSE_DO_READDIRPLUS
				| sys::FUSE_ASYNC_READ
				| sys::FUSE_PARALLEL_DIROPS
				| sys::FUSE_CACHE_SYMLINKS
				| sys::FUSE_SPLICE_READ
				| sys::FUSE_SPLICE_MOVE
				| sys::FUSE_NO_OPENDIR_SUPPORT
				| sys::FUSE_INIT_EXT,
		);
		if passthrough {
			flags |= sys::FUSE_PASSTHROUGH;
		}
		flags |= sys::FUSE_OVER_IO_URING;

		// Only request features the kernel supports.
		flags &= kernel_flags;
		let flags_lo = (flags & 0xFFFF_FFFF) as u32;
		let flags_hi = (flags >> 32) as u32;

		let response = fuse_init_out {
			major: FUSE_KERNEL_VERSION,
			minor: FUSE_KERNEL_MINOR_VERSION,
			max_readahead: 1024 * 1024,
			flags: flags_lo,
			max_background: 0,
			congestion_threshold: 0,
			max_write: 4096,
			time_gran: 0,
			max_pages: 0,
			map_alignment: 0,
			flags2: flags_hi,
			max_stack_depth: 0,
			request_timeout: 0,
			unused: [0; 11],
		};

		// Write the INIT response.
		let body = as_bytes(&response);
		let len = std::mem::size_of::<fuse_out_header>() + body.len();
		let out_header = fuse_out_header {
			unique: request.header.unique,
			len: len.to_u32().unwrap(),
			error: 0,
		};
		let header_bytes = as_bytes(&out_header);
		let iov = [IoSlice::new(header_bytes), IoSlice::new(body)];
		rustix::io::writev(fd.as_fd(), &iov)?;

		Ok(())
	}

	fn deserialize_request(buffer: &[u8]) -> Result<Request> {
		let header: fuse_in_header = read_data(buffer)?;
		let header_len = std::mem::size_of::<sys::fuse_in_header>();
		let data = &buffer[header_len..];
		let data = match header.opcode {
			sys::fuse_opcode_FUSE_BATCH_FORGET => {
				let batch_in: fuse_batch_forget_in = read_data(data)?;
				let entries_data = &data[std::mem::size_of::<fuse_batch_forget_in>()..];
				let mut forgets = Vec::with_capacity(batch_in.count as usize);
				for i in 0..batch_in.count as usize {
					let offset = i * std::mem::size_of::<fuse_forget_one>();
					let entry: fuse_forget_one = read_data(&entries_data[offset..])?;
					forgets.push(entry);
				}
				RequestData::BatchForget(forgets)
			},
			sys::fuse_opcode_FUSE_DESTROY => RequestData::Destroy,
			sys::fuse_opcode_FUSE_FLUSH => RequestData::Flush(read_data(data)?),
			sys::fuse_opcode_FUSE_FORGET => RequestData::Forget(read_data(data)?),
			sys::fuse_opcode_FUSE_GETATTR => RequestData::GetAttr(read_data(data)?),
			sys::fuse_opcode_FUSE_GETXATTR => {
				let (fuse_getxattr_in, name) =
					data.split_at(std::mem::size_of::<sys::fuse_getxattr_in>());
				let fuse_getxattr_in = read_data(fuse_getxattr_in)?;
				let name = CString::from_vec_with_nul(name.to_owned())
					.map_err(|_| Error::other("failed to deserialize request data"))?;
				RequestData::GetXattr(fuse_getxattr_in, name)
			},
			sys::fuse_opcode_FUSE_INIT => RequestData::Init(read_data(data)?),
			sys::fuse_opcode_FUSE_LISTXATTR => RequestData::ListXattr(read_data(data)?),
			sys::fuse_opcode_FUSE_LOOKUP => {
				let data = CString::from_vec_with_nul(data.to_owned())
					.map_err(|_| Error::other("failed to deserialize request data"))?;
				RequestData::Lookup(data)
			},
			sys::fuse_opcode_FUSE_OPEN => RequestData::Open(read_data(data)?),
			sys::fuse_opcode_FUSE_OPENDIR => RequestData::OpenDir(read_data(data)?),
			sys::fuse_opcode_FUSE_READ => RequestData::Read(read_data(data)?),
			sys::fuse_opcode_FUSE_READDIR => RequestData::ReadDir(read_data(data)?),
			sys::fuse_opcode_FUSE_READDIRPLUS => RequestData::ReadDirPlus(read_data(data)?),
			sys::fuse_opcode_FUSE_READLINK => RequestData::ReadLink,
			sys::fuse_opcode_FUSE_RELEASE => RequestData::Release(read_data(data)?),
			sys::fuse_opcode_FUSE_RELEASEDIR => RequestData::ReleaseDir(read_data(data)?),
			sys::fuse_opcode_FUSE_STATFS => RequestData::Statfs,
			sys::fuse_opcode_FUSE_STATX => RequestData::Statx(read_data(data)?),
			sys::fuse_opcode_FUSE_INTERRUPT => RequestData::Interrupt(read_data(data)?),
			_ => RequestData::Unsupported(header.opcode),
		};
		let request = Request { header, data };
		Ok(request)
	}

	/// Deserialize a FUSE request from a uring cmd buffer entry. The uring cmd layout splits the
	/// request across three regions: `in_out` (`fuse_in_header`), `op_in` (operation-specific struct),
	/// and the payload buffer (variable-length data like filenames).
	fn deserialize_uring_cmd_request(entry: &UringCmdEntry) -> Result<Request> {
		// Read the fuse_in_header from in_out.
		let in_out_bytes: &[u8] = unsafe {
			std::slice::from_raw_parts(
				entry.header.in_out.as_ptr().cast::<u8>(),
				sys::FUSE_URING_IN_OUT_HEADER_SZ as usize,
			)
		};
		let header: fuse_in_header = read_data(in_out_bytes)?;

		// Read the operation-specific data from op_in.
		let op_in_bytes: &[u8] = unsafe {
			std::slice::from_raw_parts(
				entry.header.op_in.as_ptr().cast::<u8>(),
				sys::FUSE_URING_OP_IN_OUT_SZ as usize,
			)
		};

		// Get the payload data (variable-length, e.g. filenames).
		let payload_sz = entry.header.ring_ent_in_out.payload_sz as usize;
		let payload = &entry.payload[..payload_sz];

		let data = match header.opcode {
			sys::fuse_opcode_FUSE_BATCH_FORGET => {
				let batch_in: fuse_batch_forget_in = read_data(op_in_bytes)?;
				let mut forgets = Vec::with_capacity(batch_in.count as usize);
				for i in 0..batch_in.count as usize {
					let offset = i * std::mem::size_of::<fuse_forget_one>();
					let entry: fuse_forget_one = read_data(&payload[offset..])?;
					forgets.push(entry);
				}
				RequestData::BatchForget(forgets)
			},
			sys::fuse_opcode_FUSE_DESTROY => RequestData::Destroy,
			sys::fuse_opcode_FUSE_FLUSH => RequestData::Flush(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_FORGET => RequestData::Forget(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_GETATTR => RequestData::GetAttr(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_GETXATTR => {
				let fuse_getxattr_in = read_data(op_in_bytes)?;
				let name = CString::from_vec_with_nul(payload.to_owned())
					.map_err(|_| Error::other("failed to deserialize request data"))?;
				RequestData::GetXattr(fuse_getxattr_in, name)
			},
			sys::fuse_opcode_FUSE_INIT => RequestData::Init(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_LISTXATTR => RequestData::ListXattr(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_LOOKUP => {
				let name = CString::from_vec_with_nul(payload.to_owned())
					.map_err(|_| Error::other("failed to deserialize request data"))?;
				RequestData::Lookup(name)
			},
			sys::fuse_opcode_FUSE_OPEN => RequestData::Open(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_OPENDIR => RequestData::OpenDir(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_READ => RequestData::Read(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_READDIR => RequestData::ReadDir(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_READDIRPLUS => RequestData::ReadDirPlus(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_READLINK => RequestData::ReadLink,
			sys::fuse_opcode_FUSE_RELEASE => RequestData::Release(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_RELEASEDIR => RequestData::ReleaseDir(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_STATFS => RequestData::Statfs,
			sys::fuse_opcode_FUSE_STATX => RequestData::Statx(read_data(op_in_bytes)?),
			sys::fuse_opcode_FUSE_INTERRUPT => RequestData::Interrupt(read_data(op_in_bytes)?),
			_ => RequestData::Unsupported(header.opcode),
		};

		Ok(Request { header, data })
	}

	fn try_handle_sync(&self, request: &Request, original_fd: &OwnedFd) -> SyncResult {
		match &request.data {
			// INIT is handled before workers start.
			RequestData::Init(..) => unreachable!(),

			// Forget requests free nodes and do not produce a response.
			RequestData::Forget(data) => {
				self.provider.forget(request.header.nodeid, data.nlookup);
				SyncResult::NoResponse
			},
			RequestData::BatchForget(forgets) => {
				let pairs: Vec<(u64, u64)> =
					forgets.iter().map(|f| (f.nodeid, f.nlookup)).collect();
				self.provider.forget_multi(&pairs);
				SyncResult::NoResponse
			},
			RequestData::Destroy | RequestData::Interrupt(..) => SyncResult::NoResponse,

			// Flush and release are trivially handled.
			RequestData::Flush(..) => SyncResult::Response(Ok(Some(Response::Flush))),
			RequestData::Release(data) => {
				self.provider.close_sync(data.fh);
				SyncResult::Response(Ok(Some(Response::Release)))
			},
			RequestData::ReleaseDir(data) => {
				self.provider.close_sync(data.fh);
				SyncResult::Response(Ok(Some(Response::ReleaseDir)))
			},

			// Try sync getattr.
			RequestData::GetAttr(..) => match self.provider.getattr_sync(request.header.nodeid) {
				Err(e) if e.raw_os_error() == Some(ENOSYS) => {
					SyncResult::NeedAsync(request.clone())
				},
				result => SyncResult::Response(result.map(|attr| {
					Some(Response::GetAttr(make_fuse_attr_out(
						request.header.nodeid,
						&attr,
					)))
				})),
			},

			// Try sync lookup.
			RequestData::Lookup(name) => {
				let Ok(name_str) = name.to_str() else {
					return SyncResult::Response(Err(Error::from_raw_os_error(ENOENT)));
				};
				match self.provider.lookup_sync(request.header.nodeid, name_str) {
					Err(e) if e.raw_os_error() == Some(ENOSYS) => {
						SyncResult::NeedAsync(request.clone())
					},
					Ok(Some(node)) => match self.provider.getattr_sync(node) {
						Err(e) if e.raw_os_error() == Some(ENOSYS) => {
							SyncResult::NeedAsync(request.clone())
						},
						Ok(attr) => {
							self.provider.increment_nlookup(node);
							SyncResult::Response(Ok(Some(Response::Lookup(make_fuse_entry_out(
								node, &attr,
							)))))
						},
						Err(e) => SyncResult::Response(Err(e)),
					},
					Ok(None) => SyncResult::Response(Err(Error::from_raw_os_error(ENOENT))),
					Err(e) => SyncResult::Response(Err(e)),
				}
			},

			// Try sync open with passthrough support.
			RequestData::Open(..) => {
				match self.provider.open_sync(request.header.nodeid) {
					Err(e) if e.raw_os_error() == Some(ENOSYS) => {
						SyncResult::NeedAsync(request.clone())
					},
					Ok((fh, backing_fd)) => {
						let mut open_flags = sys::FOPEN_NOFLUSH | sys::FOPEN_KEEP_CACHE;
						let mut backing_id = 0i32;

						// Try to set up passthrough if we have a backing fd and the feature is enabled.
						if let Some(fd) = backing_fd {
							match register_passthrough(original_fd, &fd) {
								Ok(id) => {
									open_flags |= sys::FOPEN_PASSTHROUGH;
									backing_id = id;
								},
								Err(error) => {
									tracing::trace!(%error, "passthrough registration failed, falling back");
								},
							}
						}

						SyncResult::Response(Ok(Some(Response::Open(fuse_open_out {
							fh,
							open_flags,
							backing_id,
						}))))
					},
					Err(e) => SyncResult::Response(Err(e)),
				}
			},

			// Try sync read.
			RequestData::Read(data) => {
				match self
					.provider
					.read_sync(data.fh, data.offset, data.size.to_u64().unwrap())
				{
					Err(e) if e.raw_os_error() == Some(ENOSYS) => {
						SyncResult::NeedAsync(request.clone())
					},
					Ok(bytes) => SyncResult::Response(Ok(Some(Response::Read(bytes)))),
					Err(e) => SyncResult::Response(Err(e)),
				}
			},

			// Try sync readlink.
			RequestData::ReadLink => match self.provider.readlink_sync(request.header.nodeid) {
				Err(e) if e.raw_os_error() == Some(ENOSYS) => {
					SyncResult::NeedAsync(request.clone())
				},
				Ok(target) => {
					let target = CString::new(target.as_ref()).unwrap();
					SyncResult::Response(Ok(Some(Response::ReadLink(target))))
				},
				Err(e) => SyncResult::Response(Err(e)),
			},

			// Try sync readdir/readdirplus. FUSE_NO_OPENDIR_SUPPORT is always negotiated,
			// so the kernel does not send OPENDIR and we auto-open here.
			RequestData::ReadDir(data) => {
				let handle = match self.provider.opendir_sync(request.header.nodeid) {
					Err(e) if e.raw_os_error() == Some(ENOSYS) => {
						return SyncResult::NeedAsync(request.clone());
					},
					Ok(fh) => fh,
					Err(e) => return SyncResult::Response(Err(e)),
				};
				let result = match self.provider.readdir_sync(handle) {
					Err(e) if e.raw_os_error() == Some(ENOSYS) => {
						self.provider.close_sync(handle);
						return SyncResult::NeedAsync(request.clone());
					},
					Ok(entries) => build_readdir_response(&self.provider, data, &entries, false),
					Err(e) => Err(e),
				};
				self.provider.close_sync(handle);
				SyncResult::Response(result.map(|r| Some(Response::ReadDir(r))))
			},
			RequestData::ReadDirPlus(data) => {
				let handle = match self.provider.opendir_sync(request.header.nodeid) {
					Err(e) if e.raw_os_error() == Some(ENOSYS) => {
						return SyncResult::NeedAsync(request.clone());
					},
					Ok(fh) => fh,
					Err(e) => return SyncResult::Response(Err(e)),
				};
				let result = match self.provider.readdir_sync(handle) {
					Err(e) if e.raw_os_error() == Some(ENOSYS) => {
						self.provider.close_sync(handle);
						return SyncResult::NeedAsync(request.clone());
					},
					Ok(entries) => build_readdir_response(&self.provider, data, &entries, true),
					Err(e) => Err(e),
				};
				self.provider.close_sync(handle);
				SyncResult::Response(result.map(|r| Some(Response::ReadDirPlus(r))))
			},

			// OpenDir: when FUSE_NO_OPENDIR_SUPPORT is negotiated, the kernel should not send
			// these. Handle them as a fallback.
			RequestData::OpenDir(..) => match self.provider.opendir_sync(request.header.nodeid) {
				Err(e) if e.raw_os_error() == Some(ENOSYS) => {
					SyncResult::NeedAsync(request.clone())
				},
				Ok(fh) => SyncResult::Response(Ok(Some(Response::OpenDir(fuse_open_out {
					fh,
					open_flags: sys::FOPEN_CACHE_DIR | sys::FOPEN_KEEP_CACHE,
					backing_id: 0,
				})))),
				Err(e) => SyncResult::Response(Err(e)),
			},

			// Try sync getxattr.
			RequestData::GetXattr(data, name) => {
				let Ok(name_str) = name.to_str() else {
					return SyncResult::Response(Err(Error::from_raw_os_error(ENODATA)));
				};
				match self.provider.getxattr_sync(request.header.nodeid, name_str) {
					Err(e) if e.raw_os_error() == Some(ENOSYS) => {
						SyncResult::NeedAsync(request.clone())
					},
					Ok(Some(attr)) => {
						if data.size == 0 {
							let response = fuse_getxattr_out {
								size: attr.len().to_u32().unwrap(),
								padding: 0,
							};
							SyncResult::Response(Ok(Some(Response::GetXattr(
								as_bytes(&response).to_vec(),
							))))
						} else if data.size.to_usize().unwrap() < attr.len() {
							SyncResult::Response(Err(Error::from_raw_os_error(ERANGE)))
						} else {
							SyncResult::Response(Ok(Some(Response::GetXattr(attr.into()))))
						}
					},
					Ok(None) => SyncResult::Response(Err(Error::from_raw_os_error(ENODATA))),
					Err(e) => SyncResult::Response(Err(e)),
				}
			},

			// Try sync listxattrs.
			RequestData::ListXattr(data) => {
				match self.provider.listxattrs_sync(request.header.nodeid) {
					Err(e) if e.raw_os_error() == Some(ENOSYS) => {
						SyncResult::NeedAsync(request.clone())
					},
					Ok(attrs) => {
						let attrs: Vec<u8> = attrs
							.into_iter()
							.flat_map(|s| {
								let mut s = s.into_bytes();
								s.push(0);
								s.into_iter()
							})
							.collect();
						if data.size == 0 {
							let response = fuse_getxattr_out {
								size: attrs.len().to_u32().unwrap(),
								padding: 0,
							};
							SyncResult::Response(Ok(Some(Response::ListXattr(
								as_bytes(&response).to_vec(),
							))))
						} else if data.size.to_usize().unwrap() < attrs.len() {
							SyncResult::Response(Err(Error::from_raw_os_error(ERANGE)))
						} else {
							SyncResult::Response(Ok(Some(Response::ListXattr(attrs))))
						}
					},
					Err(e) => SyncResult::Response(Err(e)),
				}
			},

			// Statfs is always handled synchronously with fixed values.
			RequestData::Statfs => {
				let out = sys::fuse_statfs_out {
					st: sys::fuse_kstatfs {
						blocks: u64::MAX / 2,
						bfree: u64::MAX / 2,
						bavail: u64::MAX / 2,
						files: u64::MAX / 2,
						ffree: u64::MAX / 2,
						bsize: 65536,
						namelen: u32::MAX,
						frsize: 1024,
						padding: 0,
						spare: [0; 6],
					},
				};
				SyncResult::Response(Ok(Some(Response::Statfs(out))))
			},

			// Statx is handled the same as getattr.
			RequestData::Statx(data) => match self.provider.getattr_sync(request.header.nodeid) {
				Err(e) if e.raw_os_error() == Some(ENOSYS) => {
					SyncResult::NeedAsync(request.clone())
				},
				Ok(attr) => {
					let attr_out = make_fuse_attr_out(request.header.nodeid, &attr);
					let out = sys::fuse_statx_out {
						attr_valid: attr_out.attr_valid,
						attr_valid_nsec: attr_out.attr_valid_nsec,
						flags: data.getattr_flags,
						spare: [0; 2],
						stat: sys::fuse_statx {
							mask: 0xffff_ffff,
							ino: attr_out.attr.ino,
							size: attr_out.attr.size,
							blocks: attr_out.attr.blocks,
							blksize: attr_out.attr.blksize,
							attributes: 0,
							nlink: attr_out.attr.nlink,
							uid: attr_out.attr.uid,
							gid: attr_out.attr.gid,
							mode: attr_out.attr.mode.to_u16().unwrap(),
							__spare0: [0],
							attributes_mask: 0xffff_ffff_ffff_ffff,
							atime: sys::fuse_sx_time {
								tv_nsec: 0,
								tv_sec: 0,
								__reserved: 0,
							},
							btime: sys::fuse_sx_time {
								tv_nsec: 0,
								tv_sec: 0,
								__reserved: 0,
							},
							mtime: sys::fuse_sx_time {
								tv_nsec: 0,
								tv_sec: 0,
								__reserved: 0,
							},
							ctime: sys::fuse_sx_time {
								tv_nsec: 0,
								tv_sec: 0,
								__reserved: 0,
							},
							rdev_major: 0,
							rdev_minor: 0,
							dev_major: 0,
							dev_minor: 0,
							__spare2: [0; 14],
						},
					};
					SyncResult::Response(Ok(Some(Response::Statx(out))))
				},
				Err(e) => SyncResult::Response(Err(e)),
			},

			// Unsupported requests return ENOSYS.
			RequestData::Unsupported(opcode) => {
				tracing::trace!(?request.header, %opcode, "unsupported request");
				SyncResult::Response(Err(Error::from_raw_os_error(ENOSYS)))
			},
		}
	}

	async fn handle_request(&self, request: Request) -> Result<Option<Response>> {
		match request.data {
			RequestData::BatchForget(data) => {
				self.handle_batch_forget_request(request.header, data).await
			},
			RequestData::Destroy => Ok(None),
			RequestData::Flush(data) => self.handle_flush_request(request.header, data).await,
			RequestData::Forget(data) => self.handle_forget_request(request.header, data).await,
			RequestData::GetAttr(data) => self.handle_get_attr_request(request.header, data).await,
			RequestData::GetXattr(data, name) => {
				self.handle_get_xattr_request(request.header, data, name)
					.await
			},
			RequestData::Init(data) => self.handle_init_request(request.header, data).await,
			RequestData::ListXattr(data) => {
				self.handle_list_xattr_request(request.header, data).await
			},
			RequestData::Lookup(data) => self.handle_lookup_request(request.header, data).await,
			RequestData::Open(data) => self.handle_open_request(request.header, data).await,
			RequestData::OpenDir(data) => self.handle_open_dir_request(request.header, data).await,
			RequestData::Read(data) => self.handle_read_request(request.header, data).await,
			RequestData::ReadDir(data) => {
				self.handle_read_dir_request(request.header, data, false)
					.await
			},
			RequestData::ReadDirPlus(data) => {
				self.handle_read_dir_request(request.header, data, true)
					.await
			},
			RequestData::ReadLink => self.handle_read_link_request(request.header).await,
			RequestData::Release(data) => self.handle_release_request(request.header, data).await,
			RequestData::ReleaseDir(data) => {
				self.handle_release_dir_request(request.header, data).await
			},
			RequestData::Statfs => self.handle_statfs_request(request.header).await,
			RequestData::Statx(data) => self.handle_statx_request(request.header, data).await,
			RequestData::Interrupt(data) => {
				self.handle_interrupt_request(request.header, data).await
			},

			RequestData::Unsupported(opcode) => {
				self.handle_unsupported_request(request.header, opcode)
					.await
			},
		}
	}

	async fn handle_batch_forget_request(
		&self,
		_header: fuse_in_header,
		forgets: Vec<fuse_forget_one>,
	) -> Result<Option<Response>> {
		let pairs: Vec<(u64, u64)> = forgets.iter().map(|f| (f.nodeid, f.nlookup)).collect();
		self.provider.forget_multi(&pairs);
		Ok(None)
	}

	async fn handle_flush_request(
		&self,
		_header: fuse_in_header,
		_request: fuse_flush_in,
	) -> Result<Option<Response>> {
		Ok(Some(Response::Flush))
	}

	async fn handle_forget_request(
		&self,
		header: fuse_in_header,
		request: fuse_forget_in,
	) -> Result<Option<Response>> {
		self.provider.forget(header.nodeid, request.nlookup);
		Ok(None)
	}

	async fn handle_get_attr_request(
		&self,
		header: fuse_in_header,
		_request: fuse_getattr_in,
	) -> Result<Option<Response>> {
		let attr = self.provider.getattr(header.nodeid).await?;
		let out = make_fuse_attr_out(header.nodeid, &attr);
		Ok(Some(Response::GetAttr(out)))
	}

	async fn handle_get_xattr_request(
		&self,
		header: fuse_in_header,
		request: fuse_getxattr_in,
		name: CString,
	) -> Result<Option<Response>> {
		let name = name
			.to_str()
			.map_err(|_| Error::from_raw_os_error(ENODATA))?;
		let attr = self
			.provider
			.getxattr(header.nodeid, name)
			.await?
			.ok_or_else(|| Error::from_raw_os_error(ENODATA))?;

		// If the request size is 0, the driver is requesting the size of the xattr.
		if request.size == 0 {
			let response = fuse_getxattr_out {
				size: attr.len().to_u32().unwrap(),
				padding: 0,
			};
			let response = as_bytes(&response).to_vec();
			Ok(Some(Response::GetXattr(response)))
		} else if request.size.to_usize().unwrap() < attr.len() {
			Err(Error::from_raw_os_error(ERANGE))
		} else {
			Ok(Some(Response::GetXattr(attr.into())))
		}
	}

	async fn handle_init_request(
		&self,
		_header: fuse_in_header,
		request: fuse_init_in,
	) -> Result<Option<Response>> {
		let response = fuse_init_out {
			major: FUSE_KERNEL_VERSION,
			minor: FUSE_KERNEL_MINOR_VERSION,
			max_readahead: request.max_readahead,
			flags: sys::FUSE_DO_READDIRPLUS,
			max_background: 0,
			congestion_threshold: 0,
			max_write: 1024 * 1024,
			time_gran: 0,
			max_pages: 0,
			map_alignment: 0,
			flags2: 0,
			max_stack_depth: 0,
			request_timeout: 0,
			unused: [0; 11],
		};
		Ok(Some(Response::Init(response)))
	}

	async fn handle_list_xattr_request(
		&self,
		header: fuse_in_header,
		request: fuse_getxattr_in,
	) -> Result<Option<Response>> {
		let attrs = self
			.provider
			.listxattrs(header.nodeid)
			.await?
			.into_iter()
			.flat_map(|s| {
				let mut s = s.into_bytes();
				s.push(0);
				s.into_iter()
			})
			.collect::<Vec<_>>();

		if request.size == 0 {
			let response = fuse_getxattr_out {
				size: attrs.len().to_u32().unwrap(),
				padding: 0,
			};
			let response = as_bytes(&response).to_vec();
			Ok(Some(Response::ListXattr(response)))
		} else if request.size.to_usize().unwrap() < attrs.len() {
			Err(Error::from_raw_os_error(ERANGE))
		} else {
			Ok(Some(Response::ListXattr(attrs)))
		}
	}

	async fn handle_lookup_request(
		&self,
		header: fuse_in_header,
		request: CString,
	) -> Result<Option<Response>> {
		let name = request
			.to_str()
			.map_err(|_| Error::from_raw_os_error(ENOENT))?;
		let node = self
			.provider
			.lookup(header.nodeid, name)
			.await?
			.ok_or_else(|| Error::from_raw_os_error(ENOENT))?;
		let attr = self.provider.getattr(node).await?;
		self.provider.increment_nlookup(node);
		let out = make_fuse_entry_out(node, &attr);
		Ok(Some(Response::Lookup(out)))
	}

	async fn handle_open_request(
		&self,
		header: fuse_in_header,
		_request: fuse_open_in,
	) -> Result<Option<Response>> {
		let fh = self.provider.open(header.nodeid).await?;
		let out = fuse_open_out {
			fh,
			open_flags: sys::FOPEN_NOFLUSH | sys::FOPEN_KEEP_CACHE,
			backing_id: 0,
		};
		Ok(Some(Response::Open(out)))
	}

	async fn handle_open_dir_request(
		&self,
		header: fuse_in_header,
		_request: fuse_open_in,
	) -> Result<Option<Response>> {
		let fh = self.provider.opendir(header.nodeid).await?;
		let out = fuse_open_out {
			fh,
			open_flags: sys::FOPEN_CACHE_DIR | sys::FOPEN_KEEP_CACHE,
			backing_id: 0,
		};
		Ok(Some(Response::OpenDir(out)))
	}

	async fn handle_read_request(
		&self,
		_header: fuse_in_header,
		request: fuse_read_in,
	) -> Result<Option<Response>> {
		let bytes = self
			.provider
			.read(request.fh, request.offset, request.size.to_u64().unwrap())
			.await?;
		Ok(Some(Response::Read(bytes)))
	}

	async fn handle_read_dir_request(
		&self,
		_header: fuse_in_header,
		request: fuse_read_in,
		plus: bool,
	) -> Result<Option<Response>> {
		let entries = self.provider.readdir(request.fh).await?;

		let struct_size = if plus {
			std::mem::size_of::<fuse_direntplus>()
		} else {
			std::mem::size_of::<fuse_dirent>()
		};

		let entries = entries
			.into_iter()
			.enumerate()
			.skip(request.offset.to_usize().unwrap());
		let mut response = Vec::with_capacity(request.size.to_usize().unwrap());
		for (offset, (name, node)) in entries {
			let attr = self.provider.getattr(node).await?;
			let name = name.into_bytes();
			let padding = (8 - (struct_size + name.len()) % 8) % 8;
			let entry_size = struct_size + name.len() + padding;
			if response.len() + entry_size > request.size.to_usize().unwrap() {
				break;
			}

			let type_ = match attr.typ {
				FileType::Directory => S_IFDIR,
				FileType::File { .. } => S_IFREG,
				FileType::Symlink => S_IFLNK,
			};

			let entry = fuse_dirent {
				ino: node,
				off: offset.to_u64().unwrap() + 1,
				namelen: name.len().to_u32().unwrap(),
				type_,
			};

			if plus {
				self.provider.increment_nlookup(node);
				let entry = fuse_direntplus {
					entry_out: make_fuse_entry_out(node, &attr),
					dirent: entry,
				};
				response.extend_from_slice(as_bytes(&entry));
			} else {
				response.extend_from_slice(as_bytes(&entry));
			}
			response.extend_from_slice(&name);
			response.extend((0..padding).map(|_| 0));
		}

		if plus {
			Ok(Some(Response::ReadDirPlus(response)))
		} else {
			Ok(Some(Response::ReadDir(response)))
		}
	}

	async fn handle_read_link_request(&self, header: fuse_in_header) -> Result<Option<Response>> {
		let target = self.provider.readlink(header.nodeid).await?;
		let target = CString::new(target.as_ref()).unwrap();
		Ok(Some(Response::ReadLink(target)))
	}

	async fn handle_release_request(
		&self,
		_header: fuse_in_header,
		request: fuse_release_in,
	) -> Result<Option<Response>> {
		self.provider.close(request.fh).await;
		Ok(Some(Response::Release))
	}

	async fn handle_release_dir_request(
		&self,
		_header: fuse_in_header,
		request: fuse_release_in,
	) -> Result<Option<Response>> {
		self.provider.close(request.fh).await;
		Ok(Some(Response::ReleaseDir))
	}

	async fn handle_statfs_request(
		&self,
		_header: sys::fuse_in_header,
	) -> Result<Option<Response>> {
		let out = sys::fuse_statfs_out {
			st: sys::fuse_kstatfs {
				blocks: u64::MAX / 2,
				bfree: u64::MAX / 2,
				bavail: u64::MAX / 2,
				files: u64::MAX / 2,
				ffree: u64::MAX / 2,
				bsize: 65536,
				namelen: u32::MAX,
				frsize: 1024,
				padding: 0,
				spare: [0; 6],
			},
		};
		Ok(Some(Response::Statfs(out)))
	}

	async fn handle_statx_request(
		&self,
		header: sys::fuse_in_header,
		request: sys::fuse_statx_in,
	) -> Result<Option<Response>> {
		let Some(Response::GetAttr(attr)) = self
			.handle_get_attr_request(header, {
				sys::fuse_getattr_in {
					getattr_flags: request.getattr_flags,
					dummy: 0,
					fh: request.fh,
				}
			})
			.await?
		else {
			return Ok(None);
		};
		let out = sys::fuse_statx_out {
			attr_valid: attr.attr_valid,
			attr_valid_nsec: attr.attr_valid_nsec,
			flags: request.getattr_flags,
			spare: [0; 2],
			stat: sys::fuse_statx {
				mask: 0xffff_ffff,
				ino: attr.attr.ino,
				size: attr.attr.size,
				blocks: attr.attr.blocks,
				blksize: attr.attr.blksize,
				attributes: 0,
				nlink: attr.attr.nlink,
				uid: attr.attr.uid,
				gid: attr.attr.gid,
				mode: attr.attr.mode.to_u16().unwrap(),
				__spare0: [0],
				attributes_mask: 0xffff_ffff_ffff_ffff,
				atime: sys::fuse_sx_time {
					tv_nsec: 0,
					tv_sec: 0,
					__reserved: 0,
				},
				btime: sys::fuse_sx_time {
					tv_nsec: 0,
					tv_sec: 0,
					__reserved: 0,
				},
				mtime: sys::fuse_sx_time {
					tv_nsec: 0,
					tv_sec: 0,
					__reserved: 0,
				},
				ctime: sys::fuse_sx_time {
					tv_nsec: 0,
					tv_sec: 0,
					__reserved: 0,
				},
				rdev_major: 0,
				rdev_minor: 0,
				dev_major: 0,
				dev_minor: 0,
				__spare2: [0; 14],
			},
		};
		Ok(Some(Response::Statx(out)))
	}

	async fn handle_interrupt_request(
		&self,
		_header: fuse_in_header,
		_request: fuse_interrupt_in,
	) -> Result<Option<Response>> {
		Ok(None)
	}

	async fn handle_unsupported_request(
		&self,
		header: fuse_in_header,
		request: u32,
	) -> Result<Option<Response>> {
		tracing::trace!(?header, %request, "unsupported request");
		Err(Error::from_raw_os_error(ENOSYS))
	}

	async fn mount(path: &Path) -> Result<Arc<OwnedFd>> {
		// Create the socket pair. Both sockets get CLOEXEC by default.
		let (sock_parent, sock_child) = rustix::net::socketpair(
			rustix::net::AddressFamily::UNIX,
			rustix::net::SocketType::STREAM,
			rustix::net::SocketFlags::CLOEXEC,
			None,
		)?;

		let child_fd_raw = sock_child.as_raw_fd();

		let uid = rustix::process::getuid().as_raw();
		let gid = rustix::process::getgid().as_raw();
		let options = format!("rootmode=40755,user_id={uid},group_id={gid},default_permissions");

		// Spawn fusermount3, passing the child socket fd via _FUSE_COMMFD.
		// Safety: The pre_exec closure only calls async-signal-safe operations.
		let mut child = unsafe {
			std::process::Command::new("fusermount3")
				.args(["-o", &options, "--"])
				.arg(path)
				.env("_FUSE_COMMFD", child_fd_raw.to_string())
				.stdin(std::process::Stdio::null())
				.stdout(std::process::Stdio::null())
				.stderr(std::process::Stdio::null())
				.pre_exec(move || {
					// Clear CLOEXEC on the child socket so fusermount3 inherits it.
					let fd = rustix::fd::BorrowedFd::borrow_raw(child_fd_raw);
					rustix::io::fcntl_setfd(fd, rustix::io::FdFlags::empty())?;
					Ok(())
				})
				.spawn()
				.map_err(|e| Error::other(format!("failed to spawn fusermount3: {e}")))?
		};

		// Close the child end in the parent.
		drop(sock_child);

		// Receive the FUSE fd from the parent socket via SCM_RIGHTS.
		let mut space = [std::mem::MaybeUninit::<u8>::uninit(); rustix::cmsg_space!(ScmRights(1))];
		let mut cmsg_buffer = rustix::net::RecvAncillaryBuffer::new(&mut space);
		let mut data_buf = [0u8; 8];
		let mut iov = [std::io::IoSliceMut::new(&mut data_buf)];
		let msg = rustix::net::recvmsg(
			&sock_parent,
			&mut iov,
			&mut cmsg_buffer,
			rustix::net::RecvFlags::empty(),
		)?;

		if msg.bytes == 0 {
			// fusermount3 closed the socket without sending an fd.
			let status = child.wait()?;
			return Err(Error::other(format!(
				"failed to read the control message from fusermount3 (exit code {})",
				status.code().unwrap_or(-1)
			)));
		}

		// Extract the fd from SCM_RIGHTS.
		let mut fuse_fd = None;
		for cmsg in cmsg_buffer.drain() {
			if let rustix::net::RecvAncillaryMessage::ScmRights(fds) = cmsg {
				for fd in fds {
					fuse_fd = Some(fd);
				}
			}
		}

		let fd = fuse_fd.ok_or_else(|| Error::other("missing the control message"))?;

		// Set CLOEXEC on the FUSE fd.
		rustix::io::fcntl_setfd(&fd, rustix::io::FdFlags::CLOEXEC)?;

		// Wait for fusermount3 to complete.
		child.wait()?;

		Ok(Arc::new(fd))
	}
}

/// Build a readdir response from a list of entries, using sync getattr.
fn build_readdir_response<P: Provider>(
	provider: &P,
	request: &fuse_read_in,
	entries: &[(String, u64)],
	plus: bool,
) -> Result<Vec<u8>> {
	let struct_size = if plus {
		std::mem::size_of::<fuse_direntplus>()
	} else {
		std::mem::size_of::<fuse_dirent>()
	};

	let entries = entries
		.iter()
		.enumerate()
		.skip(request.offset.to_usize().unwrap());
	let mut response = Vec::with_capacity(request.size.to_usize().unwrap());
	for (offset, (name, node)) in entries {
		let node = *node;
		let attr = provider.getattr_sync(node)?;
		let name = name.as_bytes();
		let padding = (8 - (struct_size + name.len()) % 8) % 8;
		let entry_size = struct_size + name.len() + padding;
		if response.len() + entry_size > request.size.to_usize().unwrap() {
			break;
		}

		let type_ = match attr.typ {
			FileType::Directory => S_IFDIR,
			FileType::File { .. } => S_IFREG,
			FileType::Symlink => S_IFLNK,
		};

		let entry = fuse_dirent {
			ino: node,
			off: offset.to_u64().unwrap() + 1,
			namelen: name.len().to_u32().unwrap(),
			type_,
		};

		if plus {
			provider.increment_nlookup(node);
			let entry = fuse_direntplus {
				entry_out: make_fuse_entry_out(node, &attr),
				dirent: entry,
			};
			response.extend_from_slice(as_bytes(&entry));
		} else {
			response.extend_from_slice(as_bytes(&entry));
		}
		response.extend_from_slice(name);
		response.extend((0..padding).map(|_| 0));
	}

	Ok(response)
}

/// Build a `fuse_attr_out` from a node id and attributes.
fn make_fuse_attr_out(node: u64, attr: &crate::Attrs) -> fuse_attr_out {
	let (size, mode) = match attr.typ {
		FileType::Directory => (0, S_IFDIR | 0o555),
		FileType::File { executable, size } => (
			size,
			S_IFREG | 0o444 | (if executable { 0o111 } else { 0o000 }),
		),
		FileType::Symlink => (0, S_IFLNK | 0o444),
	};
	fuse_attr_out {
		attr_valid: u64::MAX,
		attr_valid_nsec: 0,
		attr: fuse_attr {
			ino: node,
			size,
			blocks: 0,
			atime: attr.atime.secs,
			atimensec: attr.atime.nanos,
			mtime: attr.mtime.secs,
			mtimensec: attr.mtime.nanos,
			ctime: attr.ctime.secs,
			ctimensec: attr.ctime.nanos,
			mode,
			nlink: 1,
			uid: attr.uid,
			gid: attr.gid,
			rdev: 0,
			blksize: 512,
			flags: 0,
		},
		dummy: 0,
	}
}

/// Build a `fuse_entry_out` from a node id and attributes.
fn make_fuse_entry_out(node: u64, attr: &crate::Attrs) -> fuse_entry_out {
	let attr_out = make_fuse_attr_out(node, attr);
	fuse_entry_out {
		nodeid: node,
		generation: 0,
		entry_valid: u64::MAX,
		attr_valid: u64::MAX,
		entry_valid_nsec: 0,
		attr_valid_nsec: 0,
		attr: attr_out.attr,
	}
}

/// Register a backing file for passthrough and return the backing id.
fn register_passthrough(fuse_fd: &OwnedFd, backing_fd: &OwnedFd) -> Result<i32> {
	let map = fuse_backing_map {
		fd: backing_fd.as_raw_fd(),
		flags: 0,
		padding: 0,
	};

	// Safety: FUSE_DEV_IOC_BACKING_OPEN is a valid ioctl for the FUSE device fd.
	let backing_id = unsafe { rustix::ioctl::ioctl(fuse_fd, BackingOpenIoctl(map))? };

	Ok(backing_id)
}

/// The worker thread event loop using FUSE `io_uring` command mode. A single thread manages one
/// `io_uring` ring with `Entry128` SQEs. Buffer entries are registered on per-CPU queues. In
/// steady state, each CQE delivers a request in a buffer entry; the response is written into the
/// same buffer and a `COMMIT_AND_FETCH` SQE simultaneously commits the response and fetches the
/// next request.
fn worker_thread_loop_uring_cmd<P>(
	server: &Server<P>,
	fuse_fd: &OwnedFd,
	stop_flag: &AtomicBool,
	entries_per_queue: usize,
	runtime_handle: &tokio::runtime::Handle,
) where
	P: Provider + Send + Sync + 'static,
{
	use io_uring::{opcode, squeue, types};

	// Read the number of possible CPUs from sysfs.
	let num_cpus = num_possible_cpus();
	let total_entries = entries_per_queue * num_cpus;

	// Create the io_uring ring with Entry128 support (IORING_SETUP_SQE128). SINGLE_ISSUER tells
	// the kernel that only one thread submits SQEs, avoiding ring locking. COOP_TASKRUN avoids
	// interrupt-driven completion processing. DEFER_TASKRUN defers all task work to our
	// submit_and_wait call, avoiding cross-CPU wakeups. NO_SQARRAY removes the SQ index array
	// indirection.
	let ring_size = (u32::try_from(total_entries).unwrap() + 16)
		.next_power_of_two()
		.max(64);
	let mut ring: io_uring::IoUring<squeue::Entry128> = io_uring::IoUring::builder()
		.setup_single_issuer()
		.setup_coop_taskrun()
		.setup_defer_taskrun()
		.setup_no_sqarray()
		.build(ring_size)
		.expect("failed to create io_uring with SQE128 support");

	// Create an eventfd for async completion notification.
	let event_fd = rustix::event::eventfd(0, rustix::event::EventfdFlags::NONBLOCK)
		.expect("failed to create eventfd");

	// Try to register file descriptors with io_uring for faster kernel fd lookups. Registered
	// fds use Fixed indices instead of raw fd numbers, avoiding per-operation fd table lookups.
	// Layout: Fixed(0) = fuse_fd, Fixed(1) = event_fd.
	let use_fixed = ring
		.submitter()
		.register_files(&[fuse_fd.as_raw_fd(), event_fd.as_raw_fd()])
		.is_ok();

	// Create the async completion channel.
	let (async_sender, async_receiver) =
		crossbeam_channel::unbounded::<(usize, u64, Result<Option<Response>>)>();

	// Allocate buffer entries. Each entry is assigned to a CPU queue via round-robin.
	let mut entries: Vec<UringCmdEntry> = (0..total_entries)
		.map(|i| UringCmdEntry::new(u16::try_from(i % num_cpus).unwrap()))
		.collect();

	tracing::trace!(
		num_cpus,
		total_entries,
		entries_per_queue,
		use_fixed,
		ring_size,
		"uring cmd setup"
	);

	// Submit REGISTER SQEs for all entries to arm them with the kernel.
	for (idx, entry) in entries.iter().enumerate() {
		let sqe = build_uring_cmd_sqe(
			fuse_fd,
			use_fixed,
			sys::fuse_uring_cmd_FUSE_IO_URING_CMD_REGISTER,
			entry,
		)
		.user_data(idx as u64);
		unsafe {
			ring.submission()
				.push(&sqe)
				.expect("failed to push REGISTER SQE");
		}
	}

	let mut eventfd_poll_pending = false;

	loop {
		if stop_flag.load(Ordering::Relaxed) {
			break;
		}

		// Submit PollAdd for the eventfd if not already pending.
		if !eventfd_poll_pending {
			let sqe = if use_fixed {
				opcode::PollAdd::new(types::Fixed(1), POLLIN).build()
			} else {
				opcode::PollAdd::new(types::Fd(event_fd.as_raw_fd()), POLLIN).build()
			};
			let sqe: squeue::Entry128 = sqe.into();
			let sqe = sqe.user_data(EVENTFD_TOKEN);
			unsafe {
				ring.submission()
					.push(&sqe)
					.expect("failed to push eventfd PollAdd");
			}
			eventfd_poll_pending = true;
		}

		// Submit all pending SQEs and wait for at least one completion.
		if ring.submit_and_wait(1).is_err() {
			if stop_flag.load(Ordering::Relaxed) {
				break;
			}
			continue;
		}

		// Drain all completions.
		let cqes: Vec<io_uring::cqueue::Entry> = ring.completion().collect();

		for cqe in cqes {
			let token = cqe.user_data();
			let result = cqe.result();

			if token == EVENTFD_TOKEN {
				eventfd_poll_pending = false;

				// Clear the eventfd counter.
				let mut buf = [0u8; 8];
				rustix::io::read(&event_fd, &mut buf).ok();

				// Drain all async results and submit COMMIT_AND_FETCH for each.
				while let Ok((entry_idx, unique, result)) = async_receiver.try_recv() {
					let entry = &mut entries[entry_idx];
					write_uring_cmd_response(entry, unique, result);
					let sqe = build_uring_cmd_sqe(
						fuse_fd,
						use_fixed,
						sys::fuse_uring_cmd_FUSE_IO_URING_CMD_COMMIT_AND_FETCH,
						entry,
					)
					.user_data(entry_idx as u64);
					unsafe {
						ring.submission()
							.push(&sqe)
							.expect("failed to push COMMIT_AND_FETCH SQE");
					}
				}
			} else {
				// Buffer entry CQE (token = entry index).
				let entry_idx = token.to_usize().unwrap();

				if result < 0 {
					let errno = -result;
					if errno == rustix::io::Errno::NODEV.raw_os_error()
						|| errno == rustix::io::Errno::NOTCONN.raw_os_error()
					{
						return;
					}
					tracing::error!(errno, entry_idx, "uring cmd CQE error");
					continue;
				}

				// Deserialize the request from the uring cmd buffer.
				let entry = &mut entries[entry_idx];
				let request = match Server::<P>::deserialize_uring_cmd_request(entry) {
					Ok(r) => r,
					Err(error) => {
						tracing::error!(%error, "failed to deserialize uring cmd request");
						// Re-arm the entry with an empty response.
						write_uring_cmd_no_response(entry);
						let sqe = build_uring_cmd_sqe(
							fuse_fd,
							use_fixed,
							sys::fuse_uring_cmd_FUSE_IO_URING_CMD_COMMIT_AND_FETCH,
							entry,
						)
						.user_data(entry_idx as u64);
						unsafe {
							ring.submission()
								.push(&sqe)
								.expect("failed to push COMMIT_AND_FETCH SQE");
						}
						continue;
					},
				};

				let unique = request.header.unique;

				match server.try_handle_sync(&request, fuse_fd) {
					SyncResult::Response(result) => {
						write_uring_cmd_response(entry, unique, result);
						let sqe = build_uring_cmd_sqe(
							fuse_fd,
							use_fixed,
							sys::fuse_uring_cmd_FUSE_IO_URING_CMD_COMMIT_AND_FETCH,
							entry,
						)
						.user_data(entry_idx as u64);
						unsafe {
							ring.submission()
								.push(&sqe)
								.expect("failed to push COMMIT_AND_FETCH SQE");
						}
					},
					SyncResult::NoResponse => {
						// FORGET/BATCH_FORGET/DESTROY do not produce a FUSE response, but we
						// still need to re-arm the entry via COMMIT_AND_FETCH.
						write_uring_cmd_no_response(entry);
						let sqe = build_uring_cmd_sqe(
							fuse_fd,
							use_fixed,
							sys::fuse_uring_cmd_FUSE_IO_URING_CMD_COMMIT_AND_FETCH,
							entry,
						)
						.user_data(entry_idx as u64);
						unsafe {
							ring.submission()
								.push(&sqe)
								.expect("failed to push COMMIT_AND_FETCH SQE");
						}
					},
					SyncResult::NeedAsync(request) => {
						// Hold the entry (do not re-arm) until the async task completes.
						let server = server.clone();
						let sender = async_sender.clone();
						let event_fd_raw = event_fd.as_raw_fd();
						runtime_handle.spawn(async move {
							let result =
								server
									.handle_request(request.clone())
									.await
									.inspect_err(|error| {
										let opcode = request.header.opcode;
										if !is_expected_error(opcode, error.raw_os_error()) {
											tracing::error!(?error, ?opcode, "unexpected error");
										}
									});
							sender.send((entry_idx, unique, result)).ok();
							// Safety: The event_fd outlives this async task because the worker
							// thread joins before the server is dropped.
							let fd = unsafe { rustix::fd::BorrowedFd::borrow_raw(event_fd_raw) };
							rustix::io::write(fd, &1u64.to_ne_bytes()).ok();
						});
					},
				}
			}
		}
	}
}

/// Build an `io_uring` `URING_CMD` SQE (`Entry128`) for a FUSE uring cmd operation. The SQE's cmd
/// area contains the `fuse_uring_cmd_req`, and the addr/len fields are patched to point to the
/// entry's iov array.
fn build_uring_cmd_sqe(
	fuse_fd: &OwnedFd,
	use_fixed: bool,
	cmd_op: u32,
	entry: &UringCmdEntry,
) -> io_uring::squeue::Entry128 {
	use io_uring::{opcode, types};

	// Build the fuse_uring_cmd_req with the entry's commit_id and queue id.
	let cmd_req = sys::fuse_uring_cmd_req {
		flags: 0,
		commit_id: entry.header.ring_ent_in_out.commit_id,
		qid: entry.qid,
		padding: [0; 6],
	};

	// Serialize the cmd_req into the 80-byte cmd array.
	let mut cmd = [0u8; 80];
	let cmd_req_bytes = as_bytes(&cmd_req);
	cmd[..cmd_req_bytes.len()].copy_from_slice(cmd_req_bytes);

	// Build the Entry128 via UringCmd80.
	let mut sqe = if use_fixed {
		opcode::UringCmd80::new(types::Fixed(0), cmd_op)
			.cmd(cmd)
			.build()
	} else {
		opcode::UringCmd80::new(types::Fd(fuse_fd.as_raw_fd()), cmd_op)
			.cmd(cmd)
			.build()
	};

	// Patch the SQE's addr (offset 16) and len (offset 24) fields to point to the iov array.
	// UringCmd80 does not set these fields, but the kernel reads them to locate the buffers.
	let iov_ptr = entry.iov.as_ptr() as u64;
	unsafe {
		let sqe_ptr = (&raw mut sqe).cast::<u8>();
		std::ptr::write_unaligned(sqe_ptr.add(16).cast::<u64>(), iov_ptr);
		std::ptr::write_unaligned(sqe_ptr.add(24).cast::<u32>(), 2);
	}

	sqe
}

/// Write a FUSE response into a uring cmd buffer entry. The `fuse_out_header` is written into
/// the `in_out` region, and the response body is written into the payload buffer.
fn write_uring_cmd_response(
	entry: &mut UringCmdEntry,
	unique: u64,
	result: Result<Option<Response>>,
) {
	match result {
		Ok(Some(response)) => {
			let body = response_body(&response);
			let out_header = fuse_out_header {
				len: (std::mem::size_of::<fuse_out_header>() + body.len())
					.to_u32()
					.unwrap(),
				error: 0,
				unique,
			};
			let header_bytes = as_bytes(&out_header);
			let in_out: &mut [u8] = unsafe {
				std::slice::from_raw_parts_mut(
					entry.header.in_out.as_mut_ptr().cast::<u8>(),
					sys::FUSE_URING_IN_OUT_HEADER_SZ as usize,
				)
			};
			in_out[..header_bytes.len()].copy_from_slice(header_bytes);
			entry.payload[..body.len()].copy_from_slice(body);
			entry.header.ring_ent_in_out.payload_sz = body.len().to_u32().unwrap();
		},
		Ok(None) => {
			write_uring_cmd_no_response(entry);
		},
		Err(error) => {
			if !is_expected_error(0, error.raw_os_error()) {
				tracing::error!(?error, "unexpected error");
			}
			let errno = error.raw_os_error().unwrap_or(ENOSYS);
			let out_header = fuse_out_header {
				len: std::mem::size_of::<fuse_out_header>().to_u32().unwrap(),
				error: -errno,
				unique,
			};
			let header_bytes = as_bytes(&out_header);
			let in_out: &mut [u8] = unsafe {
				std::slice::from_raw_parts_mut(
					entry.header.in_out.as_mut_ptr().cast::<u8>(),
					sys::FUSE_URING_IN_OUT_HEADER_SZ as usize,
				)
			};
			in_out[..header_bytes.len()].copy_from_slice(header_bytes);
			entry.header.ring_ent_in_out.payload_sz = 0;
		},
	}
}

/// Write an empty response into a uring cmd buffer entry to re-arm it. Used for requests that do
/// not produce a FUSE response (FORGET, `BATCH_FORGET`, DESTROY).
fn write_uring_cmd_no_response(entry: &mut UringCmdEntry) {
	let out_header = fuse_out_header {
		len: std::mem::size_of::<fuse_out_header>().to_u32().unwrap(),
		error: 0,
		unique: 0,
	};
	let header_bytes = as_bytes(&out_header);
	let in_out: &mut [u8] = unsafe {
		std::slice::from_raw_parts_mut(
			entry.header.in_out.as_mut_ptr().cast::<u8>(),
			sys::FUSE_URING_IN_OUT_HEADER_SZ as usize,
		)
	};
	in_out[..header_bytes.len()].copy_from_slice(header_bytes);
	entry.header.ring_ent_in_out.payload_sz = 0;
}

/// Extract the response body bytes from a Response variant.
fn response_body(response: &Response) -> &[u8] {
	match response {
		Response::Flush | Response::Release | Response::ReleaseDir => &[],
		Response::GetAttr(data) => as_bytes(data),
		Response::Init(data) => as_bytes(data),
		Response::Lookup(data) => as_bytes(data),
		Response::Open(data) | Response::OpenDir(data) => as_bytes(data),
		Response::Read(data) => data,
		Response::ReadDir(data)
		| Response::ReadDirPlus(data)
		| Response::GetXattr(data)
		| Response::ListXattr(data) => data.as_slice(),
		Response::ReadLink(data) => data.as_bytes(),
		Response::Statfs(data) => as_bytes(data),
		Response::Statx(data) => as_bytes(data),
	}
}

/// Read the number of possible CPUs from sysfs. Returns at least 1.
fn num_possible_cpus() -> usize {
	let content =
		std::fs::read_to_string("/sys/devices/system/cpu/possible").unwrap_or("0-0".into());
	let content = content.trim();
	let mut max_cpu: usize = 0;
	for range in content.split(',') {
		let parts: Vec<&str> = range.split('-').collect();
		if let Some(end) = parts.last()
			&& let Ok(n) = end.parse::<usize>()
		{
			max_cpu = max_cpu.max(n);
		}
	}
	max_cpu + 1
}

/// Read a `#[repr(C)]`, `Copy` struct from the beginning of a byte slice.
fn read_data<T: Copy>(request_data: &[u8]) -> Result<T> {
	if request_data.len() < std::mem::size_of::<T>() {
		return Err(Error::other("failed to deserialize the request data"));
	}
	// Safety: T is #[repr(C)] and Copy. We verified the slice is large enough.
	Ok(unsafe { std::ptr::read_unaligned(request_data.as_ptr().cast::<T>()) })
}

/// View a `#[repr(C)]`, `Copy` struct as a byte slice.
fn as_bytes<T: Copy>(value: &T) -> &[u8] {
	// Safety: T is #[repr(C)] and Copy with no padding that could leak data.
	unsafe {
		std::slice::from_raw_parts(
			std::ptr::from_ref(value).cast::<u8>(),
			std::mem::size_of::<T>(),
		)
	}
}

impl<P> Clone for Server<P> {
	fn clone(&self) -> Self {
		Self(self.0.clone())
	}
}

impl<P> Deref for Server<P> {
	type Target = Inner<P>;

	fn deref(&self) -> &Self::Target {
		&self.0
	}
}

pub async fn unmount(path: &Path) -> Result<()> {
	let output = tokio::process::Command::new("fusermount3")
		.args(["-uz"])
		.arg(path)
		.stdin(std::process::Stdio::null())
		.stdout(std::process::Stdio::null())
		.stderr(std::process::Stdio::piped())
		.output()
		.await?;
	if !output.status.success() {
		let stderr = String::from_utf8_lossy(&output.stderr);
		return Err(Error::other(format!("failed to unmount: {stderr}")));
	}
	Ok(())
}

// There are a small number of "expected" error conditions, where the request correctly returns an error, but not due to a filesystem error. These are:
// - lookups that return ENOENT (None)
// - getxattrs that return ENODATA (None)
// - getxattr/listxattr that return ERANGE (blame the caller, they provided garbage data)
// - ENOSYS: only returned when the filesystem doesn't support a request.
fn is_expected_error(opcode: sys::fuse_opcode, error: Option<i32>) -> bool {
	(opcode == sys::fuse_opcode_FUSE_LOOKUP && error == Some(ENOENT))
		| (opcode == sys::fuse_opcode_FUSE_GETXATTR && error == Some(ENODATA))
		| (opcode == sys::fuse_opcode_FUSE_GETXATTR && error == Some(ERANGE))
		| (opcode == sys::fuse_opcode_FUSE_LISTXATTR && error == Some(ERANGE))
		| (error == Some(ENOSYS))
}
